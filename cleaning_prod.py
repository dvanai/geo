import pandas as pd
import re
from urllib.parse import urlparse
import glob
import os

# Input: all funnel response files generated by the original script
INPUT_FILES = [
    "high_funnel_responses.csv", 
    "low_funnel_responses.csv", 
    "mid_funnel_responses.csv"  # Add if you have this file
]

# Output: 4 files total
OUTPUTS = {
    "combined": "llm_table_view_combined_per_source.csv",
    "high": "llm_table_view_high_per_source.csv",
    "mid": "llm_table_view_mid_per_source.csv", 
    "low": "llm_table_view_low_per_source.csv"
}


def extract_urls(text: str) -> list[str]:
    """Extract all URLs from text using regex."""
    if not isinstance(text, str):
        return []
    pattern = r'https?://[^\s)]+'
    return re.findall(pattern, text)


def domain_from_url(url: str) -> str:
    """Get domain from URL."""
    try:
        return urlparse(url).netloc.replace('www.', '')
    except Exception:
        return ""


def process_funnel_data(df_funnel, funnel_name=None):
    """Process funnel data into per-source rows."""
    if funnel_name:
        print(f"Processing {funnel_name} funnel ({len(df_funnel)} rows)...")
    
    rows = []
    
    for idx, r in df_funnel.iterrows():
        # Skip rows with no responses
        chatgpt_response = str(r.get('chatgpt_response', ''))
        gemini_response = str(r.get('gemini_response', ''))
        
        if not chatgpt_response.strip() and not gemini_response.strip():
            continue
        
        # Process ChatGPT response
        if chatgpt_response.strip():
            urls = extract_urls(chatgpt_response)
            
            if not urls:
                rows.append({
                    "Funnel": r.get('funnel', ''),
                    "Geography": r.get('geography', ''),
                    "Category": r.get('category', ''),
                    "Prompt": r.get('original_query', ''),
                    "Full_Prompt": r.get('full_query', ''),
                    "Answer": chatgpt_response,
                    "Model": "chatgpt",
                    "Source": "",
                    "Citation": "",
                    "URL": ""
                })
            else:
                for i, url in enumerate(urls, start=1):
                    rows.append({
                        "Funnel": r.get('funnel', ''),
                        "Geography": r.get('geography', ''),
                        "Category": r.get('category', ''),
                        "Prompt": r.get('original_query', ''),
                        "Full_Prompt": r.get('full_query', ''),
                        "Answer": chatgpt_response,
                        "Model": "chatgpt",
                        "Source": domain_from_url(url),
                        #"Citation": f"[web:{i}]",
                        "Citation": f"[{i}]",
                        "URL": url
                    })
        
        # Process Gemini response (skip quota errors)
        if gemini_response.strip() and "ERROR" not in gemini_response.upper():
            urls = extract_urls(gemini_response)
            
            if not urls:
                rows.append({
                    "Funnel": r.get('funnel', ''),
                    "Geography": r.get('geography', ''),
                    "Category": r.get('category', ''),
                    "Prompt": r.get('original_query', ''),
                    "Full_Prompt": r.get('full_query', ''),
                    "Answer": gemini_response,
                    "Model": "gemini",
                    "Source": "",
                    "Citation": "",
                    "URL": ""
                })
            else:
                for i, url in enumerate(urls, start=1):
                    rows.append({
                        "Funnel": r.get('funnel', ''),
                        "Geography": r.get('geography', ''),
                        "Category": r.get('category', ''),
                        "Prompt": r.get('original_query', ''),
                        "Full_Prompt": r.get('full_query', ''),
                        "Answer": gemini_response,
                        "Model": "gemini",
                        "Source": domain_from_url(url),
                        #"Citation": f"[web:{i}]",
                        "Citation": f"[{i}]",
                        "URL": url
                    })
    
    return pd.DataFrame(rows)


# Load all funnel files
funnel_dfs = {}
all_data = []

for funnel_name in ["high", "low", "mid"]:
    files = glob.glob(f"{funnel_name}_funnel_responses.csv")
    if files:
        df = pd.read_csv(files[0])
        funnel_dfs[funnel_name] = df
        all_data.append(df)
        print(f"Loaded {funnel_name}: {len(df)} rows")
    else:
        print(f"Warning: {funnel_name}_funnel_responses.csv not found")
        funnel_dfs[funnel_name] = pd.DataFrame()

# Create combined dataset
if all_data:
    combined_df = pd.concat(all_data, ignore_index=True)
    print(f"\nCombined dataset: {len(combined_df)} total rows")
else:
    print("No input files found!")
    exit()

# Process COMBINED dataset first
print("\n=== PROCESSING COMBINED ===")
combined_out = process_funnel_data(combined_df, "Combined")
output_columns = ["Funnel", "Geography", "Category", "Prompt", "Full_Prompt", 
                 "Model", "Answer", "Source", "Citation", "URL"]
available_columns = [col for col in output_columns if col in combined_out.columns]
combined_out = combined_out[available_columns]
combined_out.to_csv(OUTPUTS["combined"], index=False)
print(f"âœ… Saved {len(combined_out)} rows to {OUTPUTS['combined']}")

# Process each individual funnel
print("\n=== PROCESSING INDIVIDUAL FUNNELS ===")
for funnel_name, df_funnel in funnel_dfs.items():
    if len(df_funnel) > 0:
        out_df = process_funnel_data(df_funnel, funnel_name.capitalize())
        
        if len(out_df) > 0:
            available_columns = [col for col in output_columns if col in out_df.columns]
            out_df = out_df[available_columns]
            
            output_file = OUTPUTS[funnel_name]
            out_df.to_csv(output_file, index=False)
            
            print(f"âœ… Saved {len(out_df)} rows to {output_file}")
            print(f"  Model breakdown: {out_df['Model'].value_counts().to_dict()}")
        else:
            print(f"No valid responses found for {funnel_name}")
    else:
        print(f"Skipping {funnel_name}: no data")

print("\nðŸŽ‰ COMPLETE! Created 4 files:")
print("  1. llm_table_view_combined_per_source.csv (ALL funnels)")
print("  2. llm_table_view_high_per_source.csv (High only)")
print("  3. llm_table_view_mid_per_source.csv (Mid only)") 
print("  4. llm_table_view_low_per_source.csv (Low only)")

# Final summary
total_output_rows = 0
for filename in OUTPUTS.values():
    if os.path.exists(filename):
        size = len(pd.read_csv(filename))
        total_output_rows += size
        print(f"  - {filename}: {size} rows")
    else:
        print(f"  - {filename}: (no data)")

print(f"\nðŸ“Š GRAND TOTAL: {total_output_rows} rows across all 4 files")
